{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "#from nltk.corpus import stopwords\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher=Matcher(nlp.vocab)\n",
    "import PyPDF2\n",
    "from pywintypes import com_error\n",
    "import win32com.client as win32\n",
    "import io\n",
    "import docx2txt\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Rohit Peter\\\\res\\\\\"\n",
    "for files in os.listdir(path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(('.pdf')):\n",
    "            abspath = os.path.join(path, filename)\n",
    "            subprocess.call('lowriter --invisible --convert-to doc \"{}\"'\n",
    "                            .format(abspath), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(document):\n",
    "    nlp_text=nlp(document)\n",
    "    pattern=[{'POS':'PROPN'},{'POS':'PROPN'}]\n",
    "    pattern1=[{'POS':'PROPN'},{'POS':'PROPN'},{'POS':'PROPN'}]\n",
    "    matcher.add('NAME',None,pattern)\n",
    "    matches=matcher(nlp_text)\n",
    "    for match_id,start,end in matches:\n",
    "        span=nlp_text[start:end]\n",
    "        if len(span.text) < 3:\n",
    "            pattern1=[{'POS':'PROPN'},{'POS':'PROPN'},{'POS':'PROPN'}]\n",
    "            matcher.add('NAME',None,pattern1)\n",
    "            matches1=matcher(nlp_text)\n",
    "            for match_id,start,end in matches1:\n",
    "                span=nlp_text[start:end]                \n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exper(fullText):\n",
    "    mi=fullText.lower()\n",
    "    #print(mi)\n",
    "    h=mi.replace(\"_\",\" \")\n",
    "    h=h.replace(\"-\",\" \")\n",
    "    h=h.replace(\",\",\" \")\n",
    "    h=h.replace(\"(\",\" \")\n",
    "    h=h.replace(\")\",\" \")\n",
    "    h=h.replace(\".docx\",\" \")\n",
    "    h=h.replace(\".pdf\",\" \")\n",
    "    h=h.split()              #look at h only years get it\n",
    "    if 'years' in h and 'months' in h:\n",
    "        if h[h.index('years')-1].isdigit() ==True:\n",
    "            d=h[h.index('years')-1] + \" \" + h[h.index('years')]+ \" \" +h[h.index('months')-1] + \" \" +h[h.index('months')]\n",
    "        else:\n",
    "            if int(h[h.index('months')-1]) >= 12:\n",
    "                d = str(int(h[h.index('months')-1])/12) + \" \" + h[h.index('years')]\n",
    "                print('yay!')\n",
    "    #elif 'year' in h:\n",
    "        #d=h[h.index('year')-1] + \" \" + h[h.index('year')]\n",
    "    elif 'years' in h:\n",
    "        d=h[h.index('years')-1] + \" \" + h[h.index('years')]\n",
    "    elif 'months' in h:\n",
    "        if int(h[h.index('months')-1]) >= 12:\n",
    "            d = str(int(h[h.index('months')-1])/12) + \" \" + h[h.index('years')]\n",
    "            #print('yay!')\n",
    "        else:\n",
    "            d =h[h.index('months')-1] + \" \" + h[h.index('months')]\n",
    "    #elif 'month' in h:\n",
    "     #   d=h[h.index('month')-1] + \" \" + h[h.index('month')]\n",
    "    #elif re.search('no experience',str(h),re.M|re.I) :\n",
    "        #d='No Experience'\n",
    "    else:\n",
    "        #d=generate_ngrams(fullText, 2)  \n",
    "        d='No Experience'\n",
    "    return d   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path, filename):\n",
    "    with open(path+filename, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            #clean_text = unicodedata.normalize(\"NFKD\",text)\n",
    "            line = ' '.join(text.split())\n",
    "            #yield clean_text\n",
    "            #yield text\n",
    "            yield line\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.append({\"Name\":'asdf',\"exp\":\"123\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'asdf', 'exp': '123'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashis_3yr_Infosys.pdf\n",
      "ASHIS PANDA 11\n",
      "3+ years\n",
      "Ayan_cv_pdf.pdf\n",
      "AYAN MAZUMDER 13\n",
      "yay!\n",
      "3.0 years\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohit Peter\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1494: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DilipBehera_Resume.pdf\n",
      "Dilip Kumar 11\n",
      "3.5 years\n",
      "Jagruti J Resume.pdf\n",
      "Jagruti Khatri 14\n",
      "No Experience\n",
      "Kuntal RoyChowdhury_Resume.pdf\n",
      "KUNTAL ROY 10\n",
      "No Experience\n",
      "Megha_Resume.pdf\n",
      "Megha Kumari 12\n",
      "No Experience\n",
      "Resume (1).pdf\n",
      "Akshay K 8\n",
      "No Experience\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Rohit Peter\\\\res\\\\'\n",
    "#data = []\n",
    "data = pd.DataFrame(columns = [\"Name\",\"Experience\"])\n",
    "n = []\n",
    "e = []\n",
    "text11 = []\n",
    "####IMPORTING PDF ONLY\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(('.pdf')):\n",
    "        print(filename)\n",
    "        res = []\n",
    "        for page in extract_text_from_pdf(path, filename):\n",
    "            \n",
    "            res +=\"\" + page\n",
    "            res = ''.join(res)\n",
    "            res = res.replace('\\n', ' ')\n",
    "            res = res.strip()\n",
    "            text11+= [res]\n",
    "            \n",
    "        name = extract_names(res)\n",
    "        print(name,len(name))\n",
    "        n+=[name]\n",
    "        exp = exper(res)\n",
    "        print(exp)\n",
    "        e+=[exp]\n",
    "        #data.append({\"Name\":name,\"Experience\": exp},ignore_index=True)\n",
    "        data._update_inplace(data.append([[name,exp]]))\n",
    "        df = pd.DataFrame(list(zip(n,e)), columns = [\"Name\",\"Experience\"])    \n",
    "        df.to_csv(\"resumes_filter10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASHIS PANDA</td>\n",
       "      <td>3+ years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AYAN MAZUMDER</td>\n",
       "      <td>3.0 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dilip Kumar</td>\n",
       "      <td>3.5 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jagruti Khatri</td>\n",
       "      <td>No Experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KUNTAL ROY</td>\n",
       "      <td>No Experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Megha Kumari</td>\n",
       "      <td>No Experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Akshay K</td>\n",
       "      <td>No Experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name     Experience\n",
       "0     ASHIS PANDA       3+ years\n",
       "1   AYAN MAZUMDER      3.0 years\n",
       "2     Dilip Kumar      3.5 years\n",
       "3  Jagruti Khatri  No Experience\n",
       "4      KUNTAL ROY  No Experience\n",
       "5    Megha Kumari  No Experience\n",
       "6        Akshay K  No Experience"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5/27/2019 ASHIS PANDA DATA ANALYST ashisk.akp@gmail.com +917358270790 ashiskumarpanda \\uf0e0 \\uf0ac https://medium.com/@GeneAshis \\uf095 \\uf099 \\uf0e1 https://www.linkedin.com/in/ashis- panda-b211b592/ CaptainAshis \\uf09b Summary Data Scientist with 3+ years of Experience executing data-driven solution to increase e\\x00ciency and accuracy. Experienced in creating machine learning models using predictive data modelling techniques and analyzing the output of the algorithm to deliver insights and implement action oriented solutions to complex business problems. Skills PYTHON SQL R DATA SCIENCE MACHINE LEARNING ALGORITHMS TABLEAU PYTORCH KERAS MATPLOTLIB NUMPY PANDAS DEEP LEARNING SCIKIT LEARN https://resume.creddle.io Education Creddle | Resume Bachelor Of Technology in Electrical and Electronics Engineering at Vellore Institute of Technology , India 2011 to 2015 Employment Infosys Analyst Projects Bangalore Dec. 2015 to Current Developed the Dynamic pricing module for one of the largest Logistics Company in USA Our Client is a Freight company who acts as a broker between Clients who wants their loads to be transported and Truckers. They would get a quotation price by their Clients for the load to be transported and would negotiate that amount with truckers. This was basically done by the Carrier Representatives in our Client company by using their existing domain knowledge and their years of experience in negotiating. Our main aim was to develop a dynamic pricing model which understands that given the factors involved, it can exactly come up with a price that has to be paid to the truckers. Responsibilities undertaken as a part of this project :- i) Developed the conceptual and analytical framework of the proposal, as well as the codes for the algorithm used for analysis. ii) Implemented various ML model for all the lanes and the data was sequenced temporally. iii) Used Random Forest , XgBoost , Ridge and Lasso models and took an ensemble approach on top of these models, to come up with the best model. iv) Worked directly on the client presentation as well as presented it to the client for the pitch. v) Trained other team members on the tools and techniques used for developing the Ensemble model. Recommendation System I developed a Recommendation system using Neural Network. 1. A recommendation system, in general consists of a User and an Item. 2. Each user and each Item were randomly initialized with a set of random numbers also known as Entity Embedding. These embeddings were passed as an input to a Neural Network and it gets updated as we train our Neural Network. 3. At the end of our training, it’s found that the User and Movie embedding which were random values initially, has been updated during the training phase and these values starts making sense. These updated set of numbers represents the genre of Movies and the Preference of Users. 4. Using these updated Entity Embedding a Recommendation system was developed. Activities Towards Data Science and Hackernoon publication. · Writer I work with Towards Data Science and Hackernoon publication as a Technical Writer. These are the biggest Machine Learning publication with an outreach of more than 100k subscribers. A couple of my blogs have received good appreciation from eminent deep learning personalities like Jeremy Howard (Deep Learning Researcher , President of Kaggle) and David Robinson (Chief Data Scientist at Datacamp) . 2018 to Current Yes Bank · Guest Blogger YesBank selected my blog to cover about their mega Datathon event held at Bangalore. Analytics Society Meetup I am a part of Analytics Society Meetup at my workplace. My responsibilities involve brainstorming concepts of Deep Learning and making the participants understand my experience of solving a project using Deep Learning approach. Guest Lecture at Praxis Business School, Bangalore · I was invited for a session on Deep Learning at Praxis Business School, Bangalore . 1/1',\n",
       " 'AYAN MAZUMDER mazuayan@gmail.com Contact No: 7829657213 Current Location: Mumbai (open to reallocation) ----------------------------------------------------------------------------------------------------------------------------- ---------- FUNCTIONAL/TECHNICAL KNOWLEDGE: • Skill Sets (Tools): SQL, R, Power BI, Qlikview, MS Excel, MS power point, SAS (base) • Analytics Modules: Predictive modelling, logistic regression, linear regression, segmentation, K means clustering, CLTV, campaign analytics, decision trees, CHAID, factor analysis, market mix modelling, time series, forecasting, credit-risk, Data Science, retail analytics, BFSI WORK EXPERIENCE Summary: Have over 36 months of analytics experience, implementing end to end analytics driven business solutions for major Automotive, Retail, DTH and BFSI clients. Tenure: - Senior Business Analyst (Analytics) in Hansa Cequity Pvt. Ltd. (April 2019- till date) - Business Analyst (Analytics) in Hansa Cequity Pvt. Ltd. (Nov 2016-March 2019) - Analytics Analyst in ACCENTURE AI (CAMPUS HIRE) (Aug 2015- March 2016) PROFESSIONAL PROJECTS @ Hansa Cequity: Segmentation of Customer Base for enriching customer experience and targeted marketing for a leading automotive industry • Performed an end to end value based customer segmentation to differentiate a high valued customer from a middle or lower valued customer, based on historical transactions. Methodology included extensive data preparation/manipulation in SQL and implementing an unsupervised clustering algorithm in R to derive the final segments. • Use Cases: Implemented these segments across all business touchpoints for differential customer treatment and increasing sales through precise communications and targeted marketing',\n",
       " 'AYAN MAZUMDER mazuayan@gmail.com Contact No: 7829657213 Current Location: Mumbai (open to reallocation) ----------------------------------------------------------------------------------------------------------------------------- ---------- FUNCTIONAL/TECHNICAL KNOWLEDGE: • Skill Sets (Tools): SQL, R, Power BI, Qlikview, MS Excel, MS power point, SAS (base) • Analytics Modules: Predictive modelling, logistic regression, linear regression, segmentation, K means clustering, CLTV, campaign analytics, decision trees, CHAID, factor analysis, market mix modelling, time series, forecasting, credit-risk, Data Science, retail analytics, BFSI WORK EXPERIENCE Summary: Have over 36 months of analytics experience, implementing end to end analytics driven business solutions for major Automotive, Retail, DTH and BFSI clients. Tenure: - Senior Business Analyst (Analytics) in Hansa Cequity Pvt. Ltd. (April 2019- till date) - Business Analyst (Analytics) in Hansa Cequity Pvt. Ltd. (Nov 2016-March 2019) - Analytics Analyst in ACCENTURE AI (CAMPUS HIRE) (Aug 2015- March 2016) PROFESSIONAL PROJECTS @ Hansa Cequity: Segmentation of Customer Base for enriching customer experience and targeted marketing for a leading automotive industry • Performed an end to end value based customer segmentation to differentiate a high valued customer from a middle or lower valued customer, based on historical transactions. Methodology included extensive data preparation/manipulation in SQL and implementing an unsupervised clustering algorithm in R to derive the final segments. • Use Cases: Implemented these segments across all business touchpoints for differential customer treatment and increasing sales through precise communications and targeted marketingCalculating CLTV for a leading automotive industry • Built a mathematical algorithm plugging in statistical models to calculate the lifetime value(past+future) of a customer to understand his business value. • Use Cases: Implemented CLTV scores across all business touchpoints to make impactful business communications with the customers thereby increasing retention and sales. Repurchase and Referral models for a leading automotive industry: • Built and implemented a repurchase model (Logistic regression model) to track down customers with a high propensity to repurchase/next purchase. • Built and implemented a referral model (Logistic regression model) to identify customers with a high propensity to refer. • Benefits: Significant increase in loyalty/referral sales and campaign cost optimization through precise marketing Lapsation and Revival Models for a leading Life Insurance client: • Built and implemented separate logistic regression models to identify customers who are most likely to lapse their policies and customers who are most likely to revive their policies • Benefits: Helps client to proactively design strategy to enhance value and retain customers. Sending campaigns to the right audience helps in cost optimization Churn Model for automotive, retail and DTH clients: • Built an end to end churn model (Logistic regression model) to track down customers with a high propensity to churn • Use Cases and Benefits: Sending customized campaigns to highly probable to churn customers to increase customer retention Projects @ Accenture: Market Mix Modeling for A LEADING TELECOM COMPANY • Scored Market Mix Models to find out the most efficient marketing source as in TV, Radio, Newspaper, Campaigns for optimizing advertising budget.',\n",
       " 'AYAN MAZUMDER mazuayan@gmail.com Contact No: 7829657213 Current Location: Mumbai (open to reallocation) ----------------------------------------------------------------------------------------------------------------------------- ---------- FUNCTIONAL/TECHNICAL KNOWLEDGE: • Skill Sets (Tools): SQL, R, Power BI, Qlikview, MS Excel, MS power point, SAS (base) • Analytics Modules: Predictive modelling, logistic regression, linear regression, segmentation, K means clustering, CLTV, campaign analytics, decision trees, CHAID, factor analysis, market mix modelling, time series, forecasting, credit-risk, Data Science, retail analytics, BFSI WORK EXPERIENCE Summary: Have over 36 months of analytics experience, implementing end to end analytics driven business solutions for major Automotive, Retail, DTH and BFSI clients. Tenure: - Senior Business Analyst (Analytics) in Hansa Cequity Pvt. Ltd. (April 2019- till date) - Business Analyst (Analytics) in Hansa Cequity Pvt. Ltd. (Nov 2016-March 2019) - Analytics Analyst in ACCENTURE AI (CAMPUS HIRE) (Aug 2015- March 2016) PROFESSIONAL PROJECTS @ Hansa Cequity: Segmentation of Customer Base for enriching customer experience and targeted marketing for a leading automotive industry • Performed an end to end value based customer segmentation to differentiate a high valued customer from a middle or lower valued customer, based on historical transactions. Methodology included extensive data preparation/manipulation in SQL and implementing an unsupervised clustering algorithm in R to derive the final segments. • Use Cases: Implemented these segments across all business touchpoints for differential customer treatment and increasing sales through precise communications and targeted marketingCalculating CLTV for a leading automotive industry • Built a mathematical algorithm plugging in statistical models to calculate the lifetime value(past+future) of a customer to understand his business value. • Use Cases: Implemented CLTV scores across all business touchpoints to make impactful business communications with the customers thereby increasing retention and sales. Repurchase and Referral models for a leading automotive industry: • Built and implemented a repurchase model (Logistic regression model) to track down customers with a high propensity to repurchase/next purchase. • Built and implemented a referral model (Logistic regression model) to identify customers with a high propensity to refer. • Benefits: Significant increase in loyalty/referral sales and campaign cost optimization through precise marketing Lapsation and Revival Models for a leading Life Insurance client: • Built and implemented separate logistic regression models to identify customers who are most likely to lapse their policies and customers who are most likely to revive their policies • Benefits: Helps client to proactively design strategy to enhance value and retain customers. Sending campaigns to the right audience helps in cost optimization Churn Model for automotive, retail and DTH clients: • Built an end to end churn model (Logistic regression model) to track down customers with a high propensity to churn • Use Cases and Benefits: Sending customized campaigns to highly probable to churn customers to increase customer retention Projects @ Accenture: Market Mix Modeling for A LEADING TELECOM COMPANY • Scored Market Mix Models to find out the most efficient marketing source as in TV, Radio, Newspaper, Campaigns for optimizing advertising budget.Market Mix Model for a leading TOY MANUFACTURING COMPANY(retail): • Measured the promotional uplifts as in the difference between baseline and actual sales to find out the most effective promotional campaigns that were generating a higher lift in sales EDUCATION: • Masters in Economics from Jadavpur University, Kolkata (2015) - GPA 7.9 • Bachelors in Economics from Jadavpur University, Kolkata (2013) - GPA 6.33 • HSC/12th Higher Secondary from South Point High School, Kolkata (2009) – 71.8% • SSC/10th Madhyamik from South Point High School, Kolkata (2007) – 84.5% NOTEWORTHY MILESTONES • Consistently rated among the top performers over the last two Financial Years • Won 6 quarterly awards at Hansa Cequity for excellence in performance • Have won several awards for acing Table Tennis and Swimming competitions in both college and school',\n",
       " 'Dilip Kumar Behera +91-8895471553 | E: dilipbehera003@gmail.com Mahadevapura, Bangalore, Karnataka 560037 PROFESSIONAL SUMMARY SKILLS Business outcome oriented data scientist with 3.5 years of experience and a proven ability of delivering powerful business insights via data science methodologies • Python: • Tableau:Tableau Numpy,Pandas,Matplotlib,Seaborn Desktop,Tableau Prep ,Scikit-learn,NLTK • Statistics • Machine Learning Algorithms • Advance Excel Classification, Regression, Clustering, CART, Dimension Reduction Techniques, NLP and Ensemble Methods • SQL WORK HISTORY DATA SCIENTIST 10/2015 to CURRENT Ericsson India Global Services | Bengaluru, KA 1.Project Summary :Sleeping Cell Predictions • Build a classification model on given historical data to analyze the traffic on individual node and predicted the sleeping nodes. • Approach: Performed data preprocessing ,exploratory data analysis after gathering data from multiple BU .Applied Logistic regression,LDA, KNN ,SVM and Random Forest Algorithms. 2.Project Summary : Field Dispatch Prediction • Built a classification model for the tickets with issue and predict whether the ticket needs to be dispatched or not • Approach:Performed data cleaning, Feature engineering and developed a classifier using Bayes Theorem and NLTK package , also improved the performance using ensemble method. 3.Project Summary: KPI Analysis And Optimization • Developed a model for predicting the network key performance Indicators (Accessibility, Retainability, Mobility, RSSI) for Sprint • Approach:Performed exploratory data alalysis and then spot checked various regression algorithms such as linear regression ,Ridge regression ,KNN ,Elastic net regression and SVM 4.Project Summary: Dashboards and Reporting • Reported insights gathered from models through Tableau across internal and external stakeholders. • Created a customized dashboard for quick KPI check after parameters changes which saved 2520 minutes yearly.',\n",
       " 'Dilip Kumar Behera +91-8895471553 | E: dilipbehera003@gmail.com Mahadevapura, Bangalore, Karnataka 560037 PROFESSIONAL SUMMARY SKILLS Business outcome oriented data scientist with 3.5 years of experience and a proven ability of delivering powerful business insights via data science methodologies • Python: • Tableau:Tableau Numpy,Pandas,Matplotlib,Seaborn Desktop,Tableau Prep ,Scikit-learn,NLTK • Statistics • Machine Learning Algorithms • Advance Excel Classification, Regression, Clustering, CART, Dimension Reduction Techniques, NLP and Ensemble Methods • SQL WORK HISTORY DATA SCIENTIST 10/2015 to CURRENT Ericsson India Global Services | Bengaluru, KA 1.Project Summary :Sleeping Cell Predictions • Build a classification model on given historical data to analyze the traffic on individual node and predicted the sleeping nodes. • Approach: Performed data preprocessing ,exploratory data analysis after gathering data from multiple BU .Applied Logistic regression,LDA, KNN ,SVM and Random Forest Algorithms. 2.Project Summary : Field Dispatch Prediction • Built a classification model for the tickets with issue and predict whether the ticket needs to be dispatched or not • Approach:Performed data cleaning, Feature engineering and developed a classifier using Bayes Theorem and NLTK package , also improved the performance using ensemble method. 3.Project Summary: KPI Analysis And Optimization • Developed a model for predicting the network key performance Indicators (Accessibility, Retainability, Mobility, RSSI) for Sprint • Approach:Performed exploratory data alalysis and then spot checked various regression algorithms such as linear regression ,Ridge regression ,KNN ,Elastic net regression and SVM 4.Project Summary: Dashboards and Reporting • Reported insights gathered from models through Tableau across internal and external stakeholders. • Created a customized dashboard for quick KPI check after parameters changes which saved 2520 minutes yearly.EDUCATION ACCOMPLISHMENTS HOBBIES • Ensured timely submission of Daily/Weekly/Monthly/Quarterly/Annual reports to respective stakeholders. B.Tech | Electronics And Telecommunications 2015 Kalinga Institute of Industrial Technology, Bhubaneswar, OD • Graduated with 8.02 CGPA • Awarded Student Leader Medal for leading our college NCC group in the republic day parade. • Named Appreciation from the Customer in Project Success Stories Publication • Power Award for Q1, 2019 • Ace Award for Q2 2018 Bodybuilding and Motorcycling']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = 'Dear Parent,\\xa0This is a test message,\\xa0kindly ignore it.\\xa0Thanks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = BeautifulSoup(tt1, \"lxml\").get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text1 =  unicodedata.normalize(\"NFKD\",tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Parent, This is a test message, kindly ignore it. Thanks'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Parent,\\xa0This is a test message,\\xa0kindly ignore it.\\xa0Thanks'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text21 = \"Asha_Resume.pdf [' Asha H R \\n\\nMobile: +91 9538044096 \\n\\nE-Mail: ashahr.jc@gmail.com \\n\\nStatistical Analysis, Reporting, Market Research and Digital Marketing \\n\\n \\nCareer Objective: \\n \\nTo secure a responsible position in Statistical Analytics/Modelling/SQL reporting where, I may utilize my professional skills and \\npersonal capabilities towards enhancing the growth of the organization. \\n \\nProfile Summary: \\n•  M.Sc. in Statistics with 3.2 years of experience in Statistical Analysis, Market Research and SQL reporting with Nielsen and \\n\\nBASES and 1 year of experience in online media reporting as a Campaign specialist with Gamut. \\n\\n•  Have proficient knowledge in analysing the data using some statistical tools like MS Excel, MS Word, MS Access PL/SQL and \\n\\nBase SAS and digital marketing tools like DFA, DFP, Flashtalking, Sizmek, Xad and TTD. \\n \\n\\nTechnical Skillset: \\n \\n•  Working knowledge on Statistics concept, SAS, PL/SQL, and SPSS  \\n•  Well versed with MS Office Suite (Word, Excel, outlook, Access& PowerPoint), Computer Hardware,  \\n• \\n•  Worked on digital marketing portals such as, DFA - Tool, Google docs, worked on other tools (DFP, DCM, Media mind report, \\n\\nProficient in creation of queries in PL/SQL with proven abilities in the work, and Knowledge about VBA. \\n\\nAtlas reports, Flashtalking, Sizmek, Xad and Innovid). \\n \\n\\nCareer Profile \\n \\nTata Consultancy Services \\n \\nDesignation \\nEmployment Period \\nDomain   \\n \\n➢  Project: Nielsen Statistical Analysis \\n \\nObjective: \\n\\n- Business Analyst \\n- Nov 2013 to Jan 2017 \\n- Retail Analytics \\n\\n \\n\\n \\n\\nThe Nielsen- world’s leading provider of marketing information, audience measurement, and business media products and services, \\nbeing a part of Nielsen our aims is to provide clients with the most complete understanding of consumers and markets worldwide. \\n \\nRoles and responsibilities: \\n\\n•  Responsible for prompt  delivery of statistical counselling and support  for internal and external clients in accordance with \\n\\n• \\n\\n• \\n• \\n• \\n• \\n• \\n• \\n\\nglobal statistical and technical standards. \\nEnsure  proper  universe  updates  and  controls  are  implemented  at  intervals  consistent  with   client  global  and  regional \\nstandards.  \\nConduct regular coverage studies of client data to ensure our trends are accurate.  \\nConduct trend analysis and forecasting provide written feedback within pre agreed timeframe.  \\nCustomer loyalty analysis based on the Scan track data \\nProviding Projection factors and inventory management reports based on seasonality co-efficient \\nSampling of stores to perform studies and customer selection techniques for offering discounts. \\nPrepared the country drossier template which includes all the process related information in one file in order to help the \\nnew associates especially to get familiar with country specifics and will help them to catch up process quickly. \\n\\n•  Reporting  all  the  activities  and  the  challenges  in  the  team  on  a  daily  basis  and  weekly  bases  to  clients  and  supervisors  by \\n\\nconducting the weekly review call \\n\\n•  As a part of client enquiries team, took the additional responsibility of investigating the data to gain an understanding of the query \\n\\nand able to provide solution to the client. \\n\\n \\n\\n\\x0c'][' ➢  Project : BASES (Booz Allen Sales Estimating System)  \\n \\nObjective: \\n\\nBASES is a sales estimation methodology for new product in branded consumer product innovation.  Nielsen‘s suite of Simulated Test \\nMarketing (STM) tools determines whether our concepts go into development and get into the hands of consumers. \\n\\n \\nRoles and responsibilities: \\n\\n• \\n• \\n\\nPerforming Campaign Analysis for the Products and Discounts offered. \\n\\nPerform Sales volume forecasting based on Value Rating, Purchase Intent, Purchase Frequency and Purchase Volume algorithm \\n\\n•  Applied statistical sampling design concepts to make good sample design to meet the client expectations i.e. enough precision, \\n\\nminimum bias and accurate picture of retail sales across globe.  \\n\\n•  Worked  for  two  completely  different  regions  i.e.  Greater  China  and  Europe  and  gain  the  market  knowledge  as  well  as  the \\n\\nconsumer behaviour considerably. Experienced the interaction of diverse background.   \\nCalculate sales effectiveness and Revenue reports for all retail departments \\nTaken the responsibility of Ad-hoc task based on client requirements \\n\\n• \\n• \\n \\nTheorem India Private Limited \\n\\n \\n\\n \\n\\nDesignation \\nEmployment Period \\nDomain   \\n\\n- Campaign Specialist \\n- Jan 05, 2017 to Jan 05, 2018 \\n\\n                  - Digital Marketing \\n\\n \\n\\n➢  Project : GAMUT \\n\\n \\n\\nObjective: \\n\\nOnline Media reporting involves gathering/highlighting information about how well a campaign/account is performing so that the \\nclient/account managers can take necessary actions to meet the goals whether it is ROI or traffic for a stipulated time/period. It is \\nused for post-advertisement, historic data representation. \\n \\n\\nRoles and Responsibilities:  \\n\\n• \\n\\nExperience in checking the billing status and fulfilling the campaigns using the first party and third party reports \\n\\n•  Working closely with the Associate Manager and assist in providing timely reporting and ensure production metrics are being \\n\\nmet on a daily basis \\n\\n• \\n\\nCreated an automation template to check the performance of the campaign and to maintain the accuracy in the report \\n\\n \\n➢  Project : Process Centralization and Standardization \\n\\n \\n\\nObjective: \\n\\nCentralized training and standardization of work leave no scope for replication of tasks or actions. \\nThis eliminates additional expenditure on excessive labour for duplication of work.  \\nData standardization, on the other hand, is the process of transforming data (available in different formats) to a standard format, \\nso as to enhance its efficiency and boost its filtering capability. \\n\\n \\n\\nRoles and Responsibilities:  \\n\\n•  Understanding & analysing the Project Requirement from client and do it in Systematic way \\n•  Handling Advertiser end reporting. \\n• \\n\\nPerforming Invoice Billing.  \\n\\n•  Quality Check for sub ordinates and coordinate and ensure the task is completed with 100% accuracy. \\n• \\n\\nTo summarize, Centralization and standardized data allows us to: Easily group and filter data for segmenting your database, \\nimprove the efficiency of customer acquisition and remain confident that your personalization is correct and that it can be \\nexecuted without worrying about errors in the data. \\n \\n\\n\\x0c'][' Achievements: \\n•  Received Team Spirit Award  \\n•  Holds the distinction of being the Star Performer in 2015 and 2016 \\n \\nAcademic Qualifications \\n \\nMaster of Science (Statistics), 2013, Department of Statistics, Manasagangothri, University of Mysore, with 7.56 GPA  \\n\\n \\nPersonal Details \\n \\nDate of Birth: \\nAddress: \\nLanguages Known: \\nMarital Status                                   - Single \\n\\n \\n\\n \\n\\n \\n\\n- February 22nd, 1989 \\n- #78 Maruthi Extension, Hunsur, Mysore District. \\n- English, Kannada and Hindi \\n\\n\\x0c'] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Asha_Resume.pdf [' Asha H R \\n\\nMobile: +91 9538044096 \\n\\nE-Mail: ashahr.jc@gmail.com \\n\\nStatistical Analysis, Reporting, Market Research and Digital Marketing \\n\\n \\nCareer Objective: \\n \\nTo secure a responsible position in Statistical Analytics/Modelling/SQL reporting where, I may utilize my professional skills and \\npersonal capabilities towards enhancing the growth of the organization. \\n \\nProfile Summary: \\n•  M.Sc. in Statistics with 3.2 years of experience in Statistical Analysis, Market Research and SQL reporting with Nielsen and \\n\\nBASES and 1 year of experience in online media reporting as a Campaign specialist with Gamut. \\n\\n•  Have proficient knowledge in analysing the data using some statistical tools like MS Excel, MS Word, MS Access PL/SQL and \\n\\nBase SAS and digital marketing tools like DFA, DFP, Flashtalking, Sizmek, Xad and TTD. \\n \\n\\nTechnical Skillset: \\n \\n•  Working knowledge on Statistics concept, SAS, PL/SQL, and SPSS  \\n•  Well versed with MS Office Suite (Word, Excel, outlook, Access& PowerPoint), Computer Hardware,  \\n• \\n•  Worked on digital marketing portals such as, DFA - Tool, Google docs, worked on other tools (DFP, DCM, Media mind report, \\n\\nProficient in creation of queries in PL/SQL with proven abilities in the work, and Knowledge about VBA. \\n\\nAtlas reports, Flashtalking, Sizmek, Xad and Innovid). \\n \\n\\nCareer Profile \\n \\nTata Consultancy Services \\n \\nDesignation \\nEmployment Period \\nDomain   \\n \\n➢  Project: Nielsen Statistical Analysis \\n \\nObjective: \\n\\n- Business Analyst \\n- Nov 2013 to Jan 2017 \\n- Retail Analytics \\n\\n \\n\\n \\n\\nThe Nielsen- world’s leading provider of marketing information, audience measurement, and business media products and services, \\nbeing a part of Nielsen our aims is to provide clients with the most complete understanding of consumers and markets worldwide. \\n \\nRoles and responsibilities: \\n\\n•  Responsible for prompt  delivery of statistical counselling and support  for internal and external clients in accordance with \\n\\n• \\n\\n• \\n• \\n• \\n• \\n• \\n• \\n\\nglobal statistical and technical standards. \\nEnsure  proper  universe  updates  and  controls  are  implemented  at  intervals  consistent  with   client  global  and  regional \\nstandards.  \\nConduct regular coverage studies of client data to ensure our trends are accurate.  \\nConduct trend analysis and forecasting provide written feedback within pre agreed timeframe.  \\nCustomer loyalty analysis based on the Scan track data \\nProviding Projection factors and inventory management reports based on seasonality co-efficient \\nSampling of stores to perform studies and customer selection techniques for offering discounts. \\nPrepared the country drossier template which includes all the process related information in one file in order to help the \\nnew associates especially to get familiar with country specifics and will help them to catch up process quickly. \\n\\n•  Reporting  all  the  activities  and  the  challenges  in  the  team  on  a  daily  basis  and  weekly  bases  to  clients  and  supervisors  by \\n\\nconducting the weekly review call \\n\\n•  As a part of client enquiries team, took the additional responsibility of investigating the data to gain an understanding of the query \\n\\nand able to provide solution to the client. \\n\\n \\n\\n\\x0c'][' ➢  Project : BASES (Booz Allen Sales Estimating System)  \\n \\nObjective: \\n\\nBASES is a sales estimation methodology for new product in branded consumer product innovation.  Nielsen‘s suite of Simulated Test \\nMarketing (STM) tools determines whether our concepts go into development and get into the hands of consumers. \\n\\n \\nRoles and responsibilities: \\n\\n• \\n• \\n\\nPerforming Campaign Analysis for the Products and Discounts offered. \\n\\nPerform Sales volume forecasting based on Value Rating, Purchase Intent, Purchase Frequency and Purchase Volume algorithm \\n\\n•  Applied statistical sampling design concepts to make good sample design to meet the client expectations i.e. enough precision, \\n\\nminimum bias and accurate picture of retail sales across globe.  \\n\\n•  Worked  for  two  completely  different  regions  i.e.  Greater  China  and  Europe  and  gain  the  market  knowledge  as  well  as  the \\n\\nconsumer behaviour considerably. Experienced the interaction of diverse background.   \\nCalculate sales effectiveness and Revenue reports for all retail departments \\nTaken the responsibility of Ad-hoc task based on client requirements \\n\\n• \\n• \\n \\nTheorem India Private Limited \\n\\n \\n\\n \\n\\nDesignation \\nEmployment Period \\nDomain   \\n\\n- Campaign Specialist \\n- Jan 05, 2017 to Jan 05, 2018 \\n\\n                  - Digital Marketing \\n\\n \\n\\n➢  Project : GAMUT \\n\\n \\n\\nObjective: \\n\\nOnline Media reporting involves gathering/highlighting information about how well a campaign/account is performing so that the \\nclient/account managers can take necessary actions to meet the goals whether it is ROI or traffic for a stipulated time/period. It is \\nused for post-advertisement, historic data representation. \\n \\n\\nRoles and Responsibilities:  \\n\\n• \\n\\nExperience in checking the billing status and fulfilling the campaigns using the first party and third party reports \\n\\n•  Working closely with the Associate Manager and assist in providing timely reporting and ensure production metrics are being \\n\\nmet on a daily basis \\n\\n• \\n\\nCreated an automation template to check the performance of the campaign and to maintain the accuracy in the report \\n\\n \\n➢  Project : Process Centralization and Standardization \\n\\n \\n\\nObjective: \\n\\nCentralized training and standardization of work leave no scope for replication of tasks or actions. \\nThis eliminates additional expenditure on excessive labour for duplication of work.  \\nData standardization, on the other hand, is the process of transforming data (available in different formats) to a standard format, \\nso as to enhance its efficiency and boost its filtering capability. \\n\\n \\n\\nRoles and Responsibilities:  \\n\\n•  Understanding & analysing the Project Requirement from client and do it in Systematic way \\n•  Handling Advertiser end reporting. \\n• \\n\\nPerforming Invoice Billing.  \\n\\n•  Quality Check for sub ordinates and coordinate and ensure the task is completed with 100% accuracy. \\n• \\n\\nTo summarize, Centralization and standardized data allows us to: Easily group and filter data for segmenting your database, \\nimprove the efficiency of customer acquisition and remain confident that your personalization is correct and that it can be \\nexecuted without worrying about errors in the data. \\n \\n\\n\\x0c'][' Achievements: \\n•  Received Team Spirit Award  \\n•  Holds the distinction of being the Star Performer in 2015 and 2016 \\n \\nAcademic Qualifications \\n \\nMaster of Science (Statistics), 2013, Department of Statistics, Manasagangothri, University of Mysore, with 7.56 GPA  \\n\\n \\nPersonal Details \\n \\nDate of Birth: \\nAddress: \\nLanguages Known: \\nMarital Status                                   - Single \\n\\n \\n\\n \\n\\n \\n\\n- February 22nd, 1989 \\n- #78 Maruthi Extension, Hunsur, Mysore District. \\n- English, Kannada and Hindi \\n\\n\\x0c'] \""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mysore District'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_names(text21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names1(document):\n",
    "    nlp_text=nlp(document)\n",
    "    #pattern=[{'POS':'PROPN'},{'POS':'PROPN'}]\n",
    "    pattern1=[{'POS':'PROPN'},{'POS':'PROPN'},{'POS':'PROPN'}]\n",
    "    matcher.add('NAME',None,pattern1)\n",
    "    matches=matcher(nlp_text)\n",
    "    for match_id,start,end in matches:\n",
    "        span=nlp_text[start:end]             \n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E-'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_names1(text21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
