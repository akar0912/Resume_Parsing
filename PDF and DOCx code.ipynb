{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Write function to compute sigmoid of a 2D array and tranform this array \n",
    "with following rule = (1 if Y>0.5) OR (0 if Y=<0.5).\n",
    "\n",
    "[[11, 12, 5, 2], [15, 6,10, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "#from nltk.corpus import stopwords\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "matcher=Matcher(nlp.vocab)\n",
    "import PyPDF2\n",
    "from pywintypes import com_error\n",
    "import win32com.client as win32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    with open(path+filename, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume_KuhuAtal_Jul'19.pdf\n",
      "[' kuhu.atal2@gmail.com | +1 (301) 732-9236 | +91 99935 00809 | 4 Baker Ave, Berkeley Heights, NJ 07922 \\n\\nhttps://www.linkedin.com/in/kuhuatal \\n\\n \\n\\nKUHU ATAL \\n\\n \\n\\n \\n\\nSUMMARY \\nAn  Information  Management  post-graduate,  with  a  specialization  in  Data  Analytics/  Data  Science  and  2.5+  years  of \\nprofessional experience across Analytics Consulting, Product and Project Management in a multicultural environment; \\nserving global clients in pharma, technology, hospitality and retail  \\n \\nEDUCATION \\nMaster of Science (M.S.), Information Management (Focus on Data Analytics)     GPA 3.94/4.0  \\nUniversity of Maryland, College of Information Studies (iSchool) \\n \\nBachelor of Engineering (B.E.), Information Technology \\nDevi Ahilya Vishwavidyalaya, Institute of Engineering and Technology \\n\\n             May ‘17 \\n      College Park, MD, US \\n\\n                                      GPA 76.9/100          \\n\\n             May ‘14 \\n      Indore, India \\n\\n    \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n   SQL, R, Python, PHP \\n   SQL Server, MySQL Workbench, Teradata, COSMOS, Datawatch Data Prep Studio \\n\\nTECHNICAL SKILLS \\nLanguages:  \\nDatabase:  \\nVisualization:     Tableau, Jigsaw, Keshif \\nOther Tools:   MS Excel, RStudio, Jupyter Notebook, Salesforce (Axtria SalesIQTM, Workbench), Vensim, LucidChart \\nStatistics: \\n\\nLinear and Logistic Regression, Clustering, Decision Trees and Random Forests, Support Vector Machines, \\nResampling Methods, Hypothesis Testing \\n\\n \\nPROFESSIONAL EXPERIENCE \\nAxtria Inc. – Senior Associate   \\n \\nBusiness Analytics Solutions and Commercial Operations \\n\\n           \\n\\n \\n\\n \\n            Oct ‘17 – Present \\n               Berkeley Heights, NJ, US \\n•  Address business needs for US and Global clients in the Commercial Excellence domain for Life Sciences industry \\n•  Apply statistical models and algorithms to provide actionable business and strategic insights \\n•  Perform ad-hoc analyses and manage end-to-end product deployment \\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n  \\n\\nProject: Incentive Compensation Data Outlier Detection  \\n\\n            Mar ‘19 – May ‘19 \\n•  Built an outlier detection model for pharmaceutical incentive compensation data based on sales anomalies using \\n\\nmachine learning in Python, to minimize the impact on a sales rep’s IC performance \\n \\n\\nProject: Global Alignment Segmentation & Targeting – Discovery & Deployment  \\n\\n             Apr ‘18 – Present \\nLed the deployment of Axtria SalesIQTM for 14 international markets (50+ countries) in the global implementation \\nof alignment, segmentation and targeting operations for a leading pharma client \\n\\n• \\n\\n•  Managed  and  carried  out  end-to-end  operations  for  a  smooth  and  timely  product  go-live;  including  market \\nonboarding (data gathering and analysis), market engagement for sales cycle setup and post go-live hypercare \\nsupport \\n\\n•  Preceded by business requirements gathering and testing (Discovery phase) \\n\\nProject: Global Incentive Compensation Harmonization  \\n\\n             Dec ‘17 – Mar ‘18 \\n•  Performed gap analysis of global incentive compensation operations on comparison with industry benchmarks \\n\\nand created brand-view and market-view visual summaries for different KPIs for a leading pharma client \\n \\n\\nNIIT Technologies Ltd. – Data Analytics Intern \\nGlobal IT Solutions  \\n\\n              Jun ‘16 – Aug ‘16 \\n Atlanta, GA, US \\n\\n \\n\\n•  Supported the development of a business intelligence platform, Digital Foresight, customized for travel and \\n\\nhospitality industry, using both structured and unstructured data \\n- \\n\\nPerformed text and sentiment analysis by extracting social media content through APIs (Twitter, Facebook) \\nand web scraping, using NLTK (Natural Language Tool Kit) in Python  \\n\\n•  Developed a Fraud Detection model for a top online retail chain by finding patterns and anomalies in bot records \\n\\nand human records in SQL and Excel, using website traffic and sales data \\n\\n\\x0c']\n",
      "[' Mu Sigma Inc. – Trainee Decision Scientist \\nData Analytics and Business Solutions \\n\\n             Sep ‘14 – Mar ‘15 \\n      Bengaluru, India \\n\\n•  Delivered regular and ad-hoc requests in the Paid Search Advertising domain for a Fortune 100 client  \\n•  Translated client requirements into business problems and processed (collect, clean) massive datasets  \\n•  Analyzed data using data modelling tools and arrived at actionable insights  \\n\\n \\nProject: LEX/CEX Analysis \\n\\n•  Analyzed the  revenue impact  and click-through-rate lift due to adoption of Location Extensions and Call \\nExtensions by advertisers on PC, Mobile and Tablet for a leading Search Engine Marketing service provider \\n \\n\\n   Project: Coupon Activity Analysis  \\n\\n•  Optimized the process of designing effective coupon campaigns by analyzing current coupon activity and \\n\\nperformance for a retail client; led a team of 4 members  \\n\\nACADEMIC PROJECTS \\nCapstone: Enhancement of User Management & Web Database Functionalities \\n\\n      \\n\\n            Sep ‘16 – May ‘17  \\n\\n•  Remodeled  website  for  a  non-profit  organization  with  an  improved  user  interface  and  enhanced  login \\n\\nsecurity using PHP and MySQL, post business requirements gathering \\n\\nVehicular Crash Analysis \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n             Sep ‘16 – Dec ‘16 \\n\\n•  Built  a  prediction  model  based  on  the  factors  affecting  accident  severity  and  frequency  of  crashes  in \\n\\nWashington D.C.; model created in Python \\n\\nStack Exchange Analysis \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n           \\n\\n \\n\\n \\n\\n            Feb ‘16 – May ‘16  \\n\\n•  Analyzed the factors which contribute to becoming potential respondents to the queries posted on Stack \\n\\nExchange and the factors which determine the quality of posts; carried out the analysis in R  \\n\\n \\n\\nBox Office Analysis (Visual Analytics)   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n            Feb ‘16 – May ‘16  \\n\\n•  Presented the analysis of Box Office 2016 (Hollywood) using Tableau Dashboard and Storytelling  \\n\\n \\n\\nAnalysis of Crimes in Bank Locations    \\n\\n \\n\\n \\n\\n \\n\\n            \\n\\n \\n\\n \\n\\n             Sep ‘15 – Dec ‘15  \\n\\n•  Analyzed the variation in crime rate with time of the day and the type of crime in locations with different \\n\\nbank densities; interpreted the results in R and Excel, and provided recommendations \\n\\nACHIEVEMENTS AND EXTRA-CURRICULARS \\n\\n•  Recognized for work with a constant high-performance rating and promotion within the first year at Axtria \\nSuccessfully  delivered  a  3-day  long  training  on  Axtria  SalesIQTM  to  30+  Sales  Force  Effectiveness \\nNational Leads from multiple countries in San Jose, Costa Rica \\n\\n- \\n\\n•  Organized DataLeague 2016, an open data Hackathon during Masters  \\n•  Served as College President during Bachelors \\n• \\n\\nInterested in dance and music – won several inter-college dance competitions; trained in Kathak \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "# calling above function and extracting text\n",
    "path = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\res\\\\test\\\\\"\n",
    "for filename in os.listdir(path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            print(filename)\n",
    "            for page in extract_text_from_pdf(path):\n",
    "                text = []\n",
    "                text.append( ' ' + page)\n",
    "                print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume_KuhuAtal_Jul'19.pdf\n",
      "kuhu.atal2@gmail.com | +1 (301) 732-9236 | +91 99935 00809 | 4 Baker Ave, Berkeley Heights, NJ 07922   https://www.linkedin.com/in/kuhuatal      KUHU ATAL         SUMMARY  An  Information  Management  post-graduate,  with  a  specialization  in  Data  Analytics/  Data  Science  and  2.5+  years  of  professional experience across Analytics Consulting, Product and Project Management in a multicultural environment;  serving global clients in pharma, technology, hospitality and retail     EDUCATION  Master of Science (M.S.), Information Management (Focus on Data Analytics)     GPA 3.94/4.0   University of Maryland, College of Information Studies (iSchool)    Bachelor of Engineering (B.E.), Information Technology  Devi Ahilya Vishwavidyalaya, Institute of Engineering and Technology                May ‘17        College Park, MD, US                                         GPA 76.9/100                         May ‘14        Indore, India                        SQL, R, Python, PHP     SQL Server, MySQL Workbench, Teradata, COSMOS, Datawatch Data Prep Studio   TECHNICAL SKILLS  Languages:   Database:   Visualization:     Tableau, Jigsaw, Keshif  Other Tools:   MS Excel, RStudio, Jupyter Notebook, Salesforce (Axtria SalesIQTM, Workbench), Vensim, LucidChart  Statistics:   Linear and Logistic Regression, Clustering, Decision Trees and Random Forests, Support Vector Machines,  Resampling Methods, Hypothesis Testing     PROFESSIONAL EXPERIENCE  Axtria Inc. – Senior Associate      Business Analytics Solutions and Commercial Operations                                 Oct ‘17 – Present                 Berkeley Heights, NJ, US  •  Address business needs for US and Global clients in the Commercial Excellence domain for Life Sciences industry  •  Apply statistical models and algorithms to provide actionable business and strategic insights  •  Perform ad-hoc analyses and manage end-to-end product deployment                      Project: Incentive Compensation Data Outlier Detection                Mar ‘19 – May ‘19  •  Built an outlier detection model for pharmaceutical incentive compensation data based on sales anomalies using   machine learning in Python, to minimize the impact on a sales rep’s IC performance     Project: Global Alignment Segmentation & Targeting – Discovery & Deployment                 Apr ‘18 – Present  Led the deployment of Axtria SalesIQTM for 14 international markets (50+ countries) in the global implementation  of alignment, segmentation and targeting operations for a leading pharma client   •   •  Managed  and  carried  out  end-to-end  operations  for  a  smooth  and  timely  product  go-live;  including  market  onboarding (data gathering and analysis), market engagement for sales cycle setup and post go-live hypercare  support   •  Preceded by business requirements gathering and testing (Discovery phase)   Project: Global Incentive Compensation Harmonization                 Dec ‘17 – Mar ‘18  •  Performed gap analysis of global incentive compensation operations on comparison with industry benchmarks   and created brand-view and market-view visual summaries for different KPIs for a leading pharma client     NIIT Technologies Ltd. – Data Analytics Intern  Global IT Solutions                  Jun ‘16 – Aug ‘16   Atlanta, GA, US      •  Supported the development of a business intelligence platform, Digital Foresight, customized for travel and   hospitality industry, using both structured and unstructured data  -   Performed text and sentiment analysis by extracting social media content through APIs (Twitter, Facebook)  and web scraping, using NLTK (Natural Language Tool Kit) in Python    •  Developed a Fraud Detection model for a top online retail chain by finding patterns and anomalies in bot records   and human records in SQL and Excel, using website traffic and sales data\n",
      "++++++++++++++++++++\n",
      "Mu Sigma Inc. – Trainee Decision Scientist  Data Analytics and Business Solutions                Sep ‘14 – Mar ‘15        Bengaluru, India   •  Delivered regular and ad-hoc requests in the Paid Search Advertising domain for a Fortune 100 client   •  Translated client requirements into business problems and processed (collect, clean) massive datasets   •  Analyzed data using data modelling tools and arrived at actionable insights      Project: LEX/CEX Analysis   •  Analyzed the  revenue impact  and click-through-rate lift due to adoption of Location Extensions and Call  Extensions by advertisers on PC, Mobile and Tablet for a leading Search Engine Marketing service provider        Project: Coupon Activity Analysis    •  Optimized the process of designing effective coupon campaigns by analyzing current coupon activity and   performance for a retail client; led a team of 4 members    ACADEMIC PROJECTS  Capstone: Enhancement of User Management & Web Database Functionalities                       Sep ‘16 – May ‘17    •  Remodeled  website  for  a  non-profit  organization  with  an  improved  user  interface  and  enhanced  login   security using PHP and MySQL, post business requirements gathering   Vehicular Crash Analysis                                        Sep ‘16 – Dec ‘16   •  Built  a  prediction  model  based  on  the  factors  affecting  accident  severity  and  frequency  of  crashes  in   Washington D.C.; model created in Python   Stack Exchange Analysis                                                 Feb ‘16 – May ‘16    •  Analyzed the factors which contribute to becoming potential respondents to the queries posted on Stack   Exchange and the factors which determine the quality of posts; carried out the analysis in R       Box Office Analysis (Visual Analytics)                                   Feb ‘16 – May ‘16    •  Presented the analysis of Box Office 2016 (Hollywood) using Tableau Dashboard and Storytelling       Analysis of Crimes in Bank Locations                                                Sep ‘15 – Dec ‘15    •  Analyzed the variation in crime rate with time of the day and the type of crime in locations with different   bank densities; interpreted the results in R and Excel, and provided recommendations   ACHIEVEMENTS AND EXTRA-CURRICULARS   •  Recognized for work with a constant high-performance rating and promotion within the first year at Axtria  Successfully  delivered  a  3-day  long  training  on  Axtria  SalesIQTM  to  30+  Sales  Force  Effectiveness  National Leads from multiple countries in San Jose, Costa Rica   -   •  Organized DataLeague 2016, an open data Hackathon during Masters   •  Served as College President during Bachelors  •   Interested in dance and music – won several inter-college dance competitions; trained in Kathak\n",
      "++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "  for filename in os.listdir(path):\n",
    "        if filename.endswith(('.pdf')):\n",
    "            print(filename)\n",
    "            for page in extract_text_from_pdf(path):\n",
    "                page = page.replace('\\n', ' ')\n",
    "                page = page.strip()\n",
    "                print(page)\n",
    "                print(\"++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "def extract_text_from_doc(path):\n",
    "    temp = docx2txt.process(path+filename)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544684526313_Resume_Akshay_P.docx\n",
      "Arpit Bhatt\n",
      "Assistant Mechanical Engineer \n",
      "   \n",
      "M: 09468517613     E: arpit9nov@gmail.com\n",
      "PERSONAL SUMMARY\n",
      "An energetic and dedicated candidate with more than 3.8 years of experience in in Business Intelligence & Analytics domain focusing on Data Science ,ETL process & visualization using Python, Tableau and Informatica power center as Primary Tools/Language seeking a suitable position in an organization having a proactive and learning culture.\n",
      " ACADEMIC QUALIFICATIONS:\n",
      "ACADEMICS\n",
      "QUALIFICATION\n",
      "UNIVERSITY/COLLEGE\n",
      "YEAR OF PASSING\n",
      "Percentage\n",
      "GRADUATION\n",
      "B.TECH        Mechanical Engineer\n",
      "Maharishi Arvind Institute of Technology, Jaipur, Rajasthan\n",
      "2014\n",
      "68.54\n",
      "TECHNICAL COMPETENCIES:\n",
      "Operating Systems\n",
      "Windows, Mac\n",
      "Programming languages\n",
      "Python, R for Data Analytics\n",
      "Tools/Utilities\n",
      "Py-charm, R-Studio, Tableau, Excel.\n",
      "WORK EXPERIENCE\n",
      "Organization: Ganga Fashions Pvt. Ltd. (3.5 years)\n",
      "Responsibilities:\n",
      "Having 3.5 years of experience in Analytics domain with expertise in providing business solutions in Functional as well as Technical approach.\n",
      "Structuring the data where ever required using python package pandas.\n",
      "Analyzing the feed using various packages such as seaborn and matplotlib.\n",
      "Performing Exploratory data analysis on Datasets.\n",
      "Experience with web crawling using selenium and Beautiful soup.\n",
      "Knowledge of machine learning models in sklearn. Worked with logistic regression.\n",
      "Automated Quarterly validations process using python.\n",
      "Extensive experience in automation, data, visualization technology.\n",
      "Experience in designing complex Operational, Strategic and Analytical Dashboards and taking advantage of all tableau functions including data blending, parameters, Hierarchies feature, sets, Forecasts etc.\n",
      "Created multiple KPI Dashboards for sales and financial analysis by providing detailed analysis on MTD/YTD/YTG actual v/s target sales\n",
      "Involved in Database testing and possess good knowledge on writing SQL queries.\n",
      "Experience in creating and working with complex SQL Queries.\n",
      "Managing Dashboards build on Tableau. Changing and maintaining logics in worksheets/dashboards and accommodating new requirements from users in existing reports.\n",
      "Well experienced in Creating Mappings/workflows using Informatica Power center.\n",
      "Involved in implementing and testing SCD type 1, SCD type 2, SCD type 3 requirements in Informatica.\n",
      "Automated Daily report creation on excel using VBA scripts.\n",
      " \n",
      "REPRESENTATIVE PROJECT EXPERIENCE:\n",
      "Project # –UAustin Animals Center \n",
      "Project Description\n",
      "Using  datasets of intake information including breed, color, sex, and age from the client, we had to predict the outcome for each animal. Also helped client to understand trends in animal outcomes. These insights helped shelters focus their energy on specific animals who need a little extra help finding a new home.\n",
      ".  \n",
      "Structuring the data where ever required using python package pandas.\n",
      "Analyzing the feed using various packages such as seaborn.\n",
      "Performing Exploratory data analysis on Datasets.\n",
      "Applying Multiclass Classification Algorithm for prediction using sklearn.\n",
      "Automated quarterly validation process Using Python.\n",
      "Project # –UK Logistics supply chain \n",
      "Project Description\n",
      "UK Based Food supply Chain Company had a requirement for analyzing which ship port would is safest for the company to be used for supplying its products across the globe. We had to take in the key factors (crime rate, illiteracy rate, average income and population size) to analyze safest port to carry out their operations.   \n",
      "Structuring the data where ever required using python package pandas.\n",
      "Analyzing the feed using various packages such as seaborn and matplotlib.\n",
      "Performing Exploratory data analysis on Datasets.\n",
      "Applying logistic regression using sklearn.\n",
      "Automated quarterly validation process Using Python.\n",
      "Project # - Schneider Reporting India\n",
      "Project Description\n",
      "Schneider Electric India had all its BI reporting operations on Microsoft excel and it was very tedious task to maintain all India reporting operations in excel and required lot of manual effort. ETL and reporting solution was provided to migrate the whole process on Tableau Reporting Tool. Worked at Client location and was involved end to end from Data modelling phase to Development of Informatica Mappings to implement Data model. \n",
      "Involved in implementing and testing SCD type 1, SCD type 2, SCD type 3 requirements in Informatica.\n",
      "Responsible for development of mappings for dimensions and facts, Data loading and code migration activities.\n",
      "Performed manual data testing using SQL Queries.  \n",
      "Managing Dashboards build on Tableau.\n",
      "Created multiple KPI Dashboards for sales and financial analysis by providing detailed analysis on MTD/YTD actual v/s target sales\n",
      "ACHIEVEMENTS & INITIATIVES: \n",
      "Received “INSTA” award for development of mappings and workflows in Schneider Electric.\n",
      "Completed Infosys certified “BI Informatica Professional -Basic” certification.\n",
      "Attended trainings for Tableau, Big Data.\n",
      "Actively participating in competitions held on kaggle for machine learning.\n",
      "PERSONAL PROFILE:\n",
      "Date of Birth:   20-12-1992\n",
      "Father’s Name:  Anil Kumar Purohit\n",
      "Languages Known: English, Hindi\n",
      "Gender: Male\n",
      "Marital Status:  Single\n",
      "4\n",
      "Abhijeet_Resume_DA (4).docx\n",
      "ABHIJEET GUPTA       +919591508918           abhi.jeet_gupta@yahoo.com\n",
      "Professional Overview:\n",
      "I have completed M.Tech from IIT Madras in Thermal Engineering with overall 4.5 Years of work experience in Mercedes Benz R & D India. An astute professional with 2 years of work experience in Data Analysis, Prediction model building, time series and 2.5 Years in Computational Fluid Dynamics (CFD)\n",
      "Summary\n",
      "  Data Analysis , provide insights and provide necessary recommendations\n",
      "  Building prediction models to predict the temperatures of specific components there by assessing the risk of not meeting the business goals\n",
      "  Data mining techniques such as Principle Component Analysis (PCA), Association Rules, Linear Regression, Logistic Regression and  Cluster Analysis \n",
      "  Machine learning algorithms such as Decision Tree, Random Forest, k-NN\n",
      "  Forecasting or time series concepts\n",
      "  Worked on R, Excel, Minitab, XL Miner and slightly on Python and SQL\n",
      "  Trained IIT-JEE students with Algebra and Statistics\n",
      "  CFD (3D) to estimates temperatures and mass flow rates of different components of vehicle\n",
      "  Performed VTM simulations using Co-simulations and CHT (Conjugate Heat Transfer)\n",
      "  Automated some of the repetitive CFD tasks Using JAVA\n",
      "Skill Set:\n",
      "Data Analysis  \n",
      "Exploratory Data analysis\n",
      "  Data mining /Machine Learning\n",
      "  Building prediction models\n",
      "  Clustering Analysis, PCA\n",
      "  Associations rules\n",
      "  Time Series using XL Miner\n",
      "  Text Mining\n",
      "  R, Python\n",
      "  Excel\n",
      "  Minitab\n",
      "Technical \n",
      "  Heat Transfer\n",
      "  Fluid Dynamics\n",
      "  C language\n",
      "  StarCCM+\n",
      "  FLUENT\n",
      "Diploma/Certifications and Positions of Responsibilities:\n",
      "3 Years of voluntary teaching experience as a IIT JEE Mathematics faculty\n",
      "Teaching Assistantship in IIT Madras for Numerical methods which involves expertise in C\n",
      "DATA ANALYTICS \n",
      "Project 1:  Data Analytics – Predict surrounding temperature of brake disc\n",
      "Tools: R and Excel\n",
      "Techniques: Linear Regression, scatter diagram, Normality check, added variable plot, VIF, Residual plots, Index plot, \n",
      "Objective: To predict surrounding temperature of brake disc\n",
      "Surrounding temperature of brake disc is dependent on temperature of brake disc and vehicle speed\n",
      "Using Multiple linear regression, surrounding prediction model was built which involves initial exploratory data analysis\n",
      "It also involved finding if input variable has any collinearity problem using scatter plot and correlation matrix\n",
      "After verifying, prediction model was built\n",
      "Coefficient of determination (R2) was found to be 86 %\n",
      "Now this model is been used to calculate surrounding temperature and hence for brake cool down analysis\n",
      "Using this prediction model there was a huge saving on computation which could have incurred on doing brake doing analysis using 3D CFD\n",
      "PROJECT 2:  Data Analytics using MACHINE LEARNING – If to accept brake disc or not\n",
      "Tools: R and Excel\n",
      "Techniques: Logistic Regression, Decision tree\n",
      "Objective: To identify those brake disc designs which are feasible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We received a data set for brake disc with input variables of different areas, volumes, heat transfer coefficient, cool down time and output variable as discrete if to accept the brake design or not based on input data \n",
      "Logistic regression model is built for this problem\n",
      "Residual deviance was found to be lesser than Null deviance which has given the indication that input variable were contributing to output variable\n",
      "Accuracy was found to be 83% using cross-table\n",
      "Similar model was also built using Decision tree\n",
      "PROJECT 3: Data Analytics using MACHINE LEARNING – Clustering Heat Exchanger\n",
      "Tools: R\n",
      "Techniques: Hierarchical clustering and K-means clustering\n",
      "Objective: To cluster different heat exchangers based on its performance and data \n",
      "Input data were In-temperature, out-temperature, mass flow rate and heat rejection through different designs of heat exchanger\n",
      "Based on given data, Euclidean distance formula was used to identify similarities for Hierarchical clustering\n",
      "K-means clustering was also implemented and found K value using elbow curve \n",
      "3D CFD PROJECTS\n",
      "PROJECT 1: Method Development on Co-simulation\n",
      "Conjugate heat transfer takes significantly more computational time \n",
      "Co-simulations takes less computational time comparatively when compare to conjugate heat transfer\n",
      "Developed an in-house macro to map fields (reference temperature, heat transfer coefficient & wall temperature) between steady fluid and moving solid \n",
      "Validated the macro for co-simulation with conjugate heat transfer results\n",
      "PROJECT 2: Vehicle Thermal Management Simulations\n",
      "It is the combination of under-hood(flow) and solid simulation  \n",
      "Under-hood considers only convection as mode of heat transfer\n",
      "Solid simulation considers both conduction and radiation modes of heat transfer\n",
      "Both Conjugate heat transfer and Co-simulations are used\n",
      "PROJECT 3: Under-hood Simulations\n",
      "Temperature distribution in the under-hood region in Car\n",
      "Automobile Under-hood consists of 8000 parts for which data is analyzed in terms of temperature\n",
      "Rotating wheels are modelled using rotating wall BC, Moving reference frame used to model fans \n",
      "Heat exchangers are modelled using porous media with inbuilt single stream heat exchanger models in STAR-CCM+\n",
      "As, it is complex geometry, surface prepping is done with the help of surface repair tool in STAR-CCM+\n",
      "Educational Qualification:\n",
      " Program\n",
      " Institution\n",
      " % or CGPA\n",
      " Year of completion\n",
      " M.Tech in Thermal Engineering\n",
      " IIT Madras, Chennai\n",
      " 8.77\n",
      " 2013\n",
      " B.E in Mechanical Engineering\n",
      " Vasavi College of Engineering,\n",
      " Osmania University, Hyderabad \n",
      " 84\n",
      " 2011\n",
      " 12th\n",
      " Vidya Vikas Junior College, Hyderabad\n",
      " 95.6\n",
      " 2007\n",
      " 10th\n",
      " Shri Gujarati Vidya Mandir High School, Hyderabad\n",
      " 81.67\n",
      " 2005\n",
      "Personal Details:\n",
      "Name: Abhijeet Gupta\n",
      "E-mail: abhi.jeet_gupta@yahoo.com\n",
      "DOB: 29th Dec, 1989\n",
      "Contact: +91 9591508918\n",
      "Language Proficiency: English, Hindi, Telugu and beginner in German and Arabic\n",
      "Permanent Address: Flat no. 209, Shree Keerthy Elite Apts, Channasandra, Whitefield, Bangalore – 560067\n",
      "DECLARATION:\n",
      "I hereby declare that all the above furnished details are true to the best of my knowledge\n",
      "Date: 01/22/2017                                                                                 ABHIJEET GUPTA\n",
      "Place: Bangalore\n",
      "Asha_Resume.doc\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'word/document.xml' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5c469f2608e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.doc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.docx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mextract_text_from_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-2bc9989aad84>\u001b[0m in \u001b[0;36mextract_text_from_doc\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_text_from_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\docx2txt\\docx2txt.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(docx, img_dir)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# get main text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mdoc_xml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'word/document.xml'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mxml2text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_xml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m# get footer text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, name, pwd)\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m         \u001b[1;34m\"\"\"Return file bytes for name.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1428\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1429\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[0;32m   1465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m             \u001b[1;31m# Get info object for name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m             \u001b[0mzinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mgetinfo\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m             raise KeyError(\n\u001b[1;32m-> 1395\u001b[1;33m                 'There is no item named %r in the archive' % name)\n\u001b[0m\u001b[0;32m   1396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"There is no item named 'word/document.xml' in the archive\""
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\res\\\\resume\\\\\"\n",
    "for filename in os.listdir(path):\n",
    "        if filename.endswith(('.doc','.docx')):\n",
    "            print(filename)\n",
    "            for page in extract_text_from_doc(path):\n",
    "                print(page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
